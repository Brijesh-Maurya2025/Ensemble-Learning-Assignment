{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-014\n",
        "# Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "ROv2QvSN804_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**"
      ],
      "metadata": {
        "id": "ghs83RDS89wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        " - Ensemble Learning is a machine learning technique where multiple models (often called “weak learners”) are combined together to solve a problem and achieve better performance than a single model.\n",
        "\n",
        "  **Key Idea Behind Ensemble Learning**\n",
        "\n",
        " - “Wisdom of the Crowd” principle → Just like a group of people can make a better decision together than any individual, a group of models can produce more accurate predictions than a single model.\n",
        "\n",
        " - The core intuition is that different models may make different errors, but when combined, their strengths can balance out the weaknesses of others."
      ],
      "metadata": {
        "id": "OcsIJCLr9EdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**"
      ],
      "metadata": {
        "id": "87KWNeI29tr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        " - Key Differences: Bagging vs Boosting\n",
        "\n",
        "  | Aspect              | **Bagging**                       | **Boosting**                                |\n",
        "| ------------------- | --------------------------------- | ------------------------------------------- |\n",
        "| **Training Style**  | Parallel (independent models)     | Sequential (each model depends on previous) |\n",
        "| **Goal**            | Reduce **variance**               | Reduce **bias** (and variance)              |\n",
        "| **Data Sampling**   | Random subsets (with replacement) | Weighted sampling (focus on mistakes)       |\n",
        "| **Model Weighting** | Equal weight to each model        | Higher weight to better models              |\n",
        "| **Overfitting**     | Handles overfitting well          | Can overfit if not tuned carefully          |\n",
        "| **Examples**        | Random Forest                     | AdaBoost, Gradient Boosting, XGBoost        |\n"
      ],
      "metadata": {
        "id": "ZL58tRoK9y1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**"
      ],
      "metadata": {
        "id": "P6MpBIPq-LCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Bootstrap Sampling**\n",
        "\n",
        " - Bootstrap sampling is a random sampling technique with replacement used to create multiple subsets of a dataset.\n",
        "\n",
        "**Role in Bagging (e.g., Random Forest)**\n",
        "\n",
        "   Bagging (Bootstrap Aggregating) relies heavily on bootstrap sampling. Here’s how it works:\n",
        "\n",
        "**1.Create Bootstrapped Datasets**\n",
        "\n",
        " - From the original dataset, generate multiple subsets using bootstrap sampling.\n",
        "\n",
        " - Each subset is slightly different from the others.\n",
        "\n",
        "**2.Train Multiple Models**\n",
        "\n",
        " - Train a weak learner (like a decision tree) on each bootstrapped dataset independently.\n",
        "\n",
        "**3.Aggregate Predictions**\n",
        "\n",
        " - For classification → majority voting.\n",
        "\n",
        " - For regression → average of predictions.\n",
        "\n",
        "**4.Effect**\n",
        "\n",
        " - Reduces variance by averaging many diverse models.\n",
        "\n",
        " - Improves stability and accuracy compared to a single model."
      ],
      "metadata": {
        "id": "sgiNB64w-XMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?**"
      ],
      "metadata": {
        "id": "vh463EJe_jRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Out-of-Bag (OOB) Samples**\n",
        "\n",
        " - When we use bootstrap sampling (sampling with replacement) to create subsets for training, not all data points are chosen.\n",
        "\n",
        " - On average, about 63% of the data ends up in each bootstrap sample.\n",
        "\n",
        " - The remaining ~37% of data points are not selected for that sample → these are called Out-of-Bag (OOB) samples.\n",
        "\n",
        " **OOB Score**\n",
        "\n",
        " - Idea: Since each tree in a Random Forest is trained on a bootstrapped sample, the OOB samples for that tree can be used as test data.\n",
        "\n",
        " - For each data point:\n",
        "\n",
        "   - Use only the trees where that data point was OOB to predict its label.\n",
        "\n",
        "   - Compare prediction with the true label.\n",
        "\n",
        " - The OOB score is simply the average accuracy (or error) across all such predictions."
      ],
      "metadata": {
        "id": "ywzm22nm_2W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n"
      ],
      "metadata": {
        "id": "1Yjw-FYCAdml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Key Differences**\n",
        "| Aspect               | **Decision Tree**                                         | **Random Forest**                              |\n",
        "| -------------------- | --------------------------------------------------------- | ---------------------------------------------- |\n",
        "| **Basis**            | Reduction in impurity from splits                         | Average reduction in impurity across all trees |\n",
        "| **Stability**        | Unstable, can vary with small data changes                | Stable, more robust due to averaging           |\n",
        "| **Bias**             | May overemphasize features with many levels (categorical) | Reduces bias by aggregating over many trees    |\n",
        "| **Interpretability** | Easier to see directly from the tree structure            | Requires aggregate importance scores           |\n"
      ],
      "metadata": {
        "id": "ssr5x0ZyAtUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "6hHDvq4aBDlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans-\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": data.feature_names,\n",
        "    \"Importance\": importances\n",
        "})\n",
        "\n",
        "# Sort by importance and get top 5\n",
        "top5_features = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "\n",
        "# Print results\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iq4WJWzBV1A",
        "outputId": "5757ad83-4561-40fc-e6df-bb2dd4e3e7dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "giHv9i7aBoTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans-\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Single Decision Tree\n",
        "# -------------------------------\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# -------------------------------\n",
        "# Bagging Classifier with Decision Trees\n",
        "# -------------------------------\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # in sklearn >=1.2 use \"estimator\" instead of \"base_estimator\"\n",
        "    n_estimators=50,   # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# -------------------------------\n",
        "# Display Results\n",
        "# -------------------------------\n",
        "print(\"=\"*40)\n",
        "print(\" Accuracy Comparison on Iris Dataset \")\n",
        "print(\"=\"*40)\n",
        "print(f\"Single Decision Tree Accuracy : {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy   : {bagging_accuracy:.4f}\")\n",
        "print(\"=\"*40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2pcrX3ZCf3e",
        "outputId": "8b32717e-b5d2-4972-8c73-d8bd78ba1484"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            " Accuracy Comparison on Iris Dataset \n",
            "========================================\n",
            "Single Decision Tree Accuracy : 1.0000\n",
            "Bagging Classifier Accuracy   : 1.0000\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy**"
      ],
      "metadata": {
        "id": "HfEXILGZDNmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans-\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris for demonstration)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 150],   # number of trees\n",
        "    \"max_depth\": [None, 3, 5, 7]      # depth of trees\n",
        "}\n",
        "\n",
        "# GridSearchCV setup\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1            # use all CPU cores\n",
        ")\n",
        "\n",
        "# Train GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# -------------------------------\n",
        "# Print Results\n",
        "# -------------------------------\n",
        "print(\"=\"*50)\n",
        "print(\" Best Hyperparameters for Random Forest \")\n",
        "print(\"=\"*50)\n",
        "print(best_params)\n",
        "print(f\"\\nFinal Accuracy on Test Set: {final_accuracy:.4f}\")\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoSkZWIKDIBd",
        "outputId": "0d2bb6e2-1c20-48d8-cfec-ba5ce3894523"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            " Best Hyperparameters for Random Forest \n",
            "==================================================\n",
            "{'max_depth': None, 'n_estimators': 100}\n",
            "\n",
            "Final Accuracy on Test Set: 1.0000\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)**"
      ],
      "metadata": {
        "id": "M_Brz71TDfbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans-\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Bagging Regressor (with Decision Tree)\n",
        "# -------------------------------\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# -------------------------------\n",
        "# Random Forest Regressor\n",
        "# -------------------------------\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# -------------------------------\n",
        "# Print Results\n",
        "# -------------------------------\n",
        "print(\"=\"*50)\n",
        "print(\" Comparison of Regressors on California Housing \")\n",
        "print(\"=\"*50)\n",
        "print(f\"Bagging Regressor MSE       : {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE : {mse_rf:.4f}\")\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbHhwVp6DZ0R",
        "outputId": "d4847b4a-75a0-4ef3-e268-ddb1df696893"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            " Comparison of Regressors on California Housing \n",
            "==================================================\n",
            "Bagging Regressor MSE       : 0.2579\n",
            "Random Forest Regressor MSE : 0.2565\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.**\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        " -  Choose between Bagging or Boosting\n",
        " -  Handle overfitting\n",
        " -  Select base models\n",
        " -  Evaluate performance using cross-validation\n",
        " - Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "rTJg0wPaDzaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans-**\n",
        "\n",
        "**Step-by-Step Approach for Loan Default Prediction with Ensemble Learning**\n",
        "\n",
        "**1. Choosing Between Bagging and Boosting**\n",
        "\n",
        " - Bagging (e.g., Random Forest): Best when the dataset has high variance and risk of overfitting (like complex decision trees). It reduces variance by averaging many models.\n",
        "\n",
        " - Boosting (e.g., XGBoost, LightGBM, AdaBoost): Best when we need higher accuracy and want to reduce both bias and variance. It sequentially focuses on misclassified/under-predicted cases.\n",
        "\n",
        "**2. Handling Overfitting**\n",
        "\n",
        " - Use cross-validation (k-fold CV) to ensure model generalization.\n",
        "\n",
        " - Apply regularization techniques available in boosting (e.g., learning_rate, max_depth, subsample, colsample_bytree).\n",
        "\n",
        " - Perform feature selection/importance analysis to remove noisy or redundant features.\n",
        "\n",
        " - Use early stopping in boosting to avoid overtraining.\n",
        "\n",
        "**3. Selecting Base Models**\n",
        "\n",
        " - For Bagging: Decision Trees are common base learners (Random Forest is essentially bagged trees).\n",
        "\n",
        " - For Boosting: Shallow Decision Trees (depth 3–6) are used as weak learners because they balance bias/variance trade-off.\n",
        "\n",
        " - Could also experiment with logistic regression or SVM as base learners in ensemble stacking.\n",
        "\n",
        "**4. Evaluating Performance with Cross-Validation**\n",
        "\n",
        " - Use Stratified k-fold Cross-Validation to preserve class distribution (important due to imbalanced loan default data).\n",
        "\n",
        " - Metrics:\n",
        "\n",
        "   - ROC-AUC (better than accuracy for imbalanced data).\n",
        "\n",
        "   - Precision-Recall AUC (to capture ability to identify defaults).\n",
        "\n",
        "   - F1-score for balance between precision & recall.\n",
        "\n",
        " - Perform hyperparameter tuning (GridSearchCV or RandomizedSearchCV) within CV.\n",
        "\n",
        "**5. Justification: Why Ensemble Learning Improves Decision-Making**\n",
        "\n",
        " - Higher Accuracy & Robustness: Combining multiple models reduces risk of poor predictions from a single model.\n",
        "\n",
        " - Bias-Variance Trade-off: Bagging reduces variance; Boosting reduces bias — both improve generalization.\n",
        "\n",
        " - Better Risk Assessment: In loan defaults, even small improvements in predictive accuracy mean millions saved in avoided bad loans.\n",
        "\n"
      ],
      "metadata": {
        "id": "qmmNmvOcE5t9"
      }
    }
  ]
}